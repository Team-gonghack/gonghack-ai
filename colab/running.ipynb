{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tBjc86bEAQqK",
        "outputId": "3d7b425c-1a75-4ca7-d2a7-83d8cf7db903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow ë²„ì „: 2.19.0\n",
            "\n",
            "--- 3-í´ë˜ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---\n",
            "ë¡œë“œëœ 'ê±·ê¸°' (ë¼ë²¨ 0) ë°ì´í„°: 3249 í–‰\n",
            "ë¡œë“œëœ 'ë›°ê¸°' (ë¼ë²¨ 1) ë°ì´í„°: 1965 í–‰\n",
            "ë¡œë“œëœ 'ì •ì§€' (ë¼ë²¨ 2) ë°ì´í„°: 2220 í–‰\n",
            "ìŠ¤ì¼€ì¼ëŸ¬ê°€ 'ê±·ê¸°+ë›°ê¸°+ì •ì§€' ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. (RobustScaler ì‚¬ìš©)\n",
            "'ê±·ê¸°' ì‹œí€€ìŠ¤ ìƒì„±: (3150, 100, 12)\n",
            "'ë›°ê¸°' ì‹œí€€ìŠ¤ ìƒì„±: (1866, 100, 12)\n",
            "'ì •ì§€' ì‹œí€€ìŠ¤ ìƒì„±: (2121, 100, 12)\n",
            "\n",
            "ì´ í•™ìŠµ ë°ì´í„°: (5709, 100, 12), ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: (1428, 100, 12)\n",
            "ë¼ë²¨ ë¶„í¬ (í•™ìŠµ): (array([0, 1, 2]), array([2504, 1493, 1712]))\n",
            "ë¼ë²¨ ë¶„í¬ (í…ŒìŠ¤íŠ¸): (array([0, 1, 2]), array([646, 373, 409]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_18 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚        \u001b[38;5;34m19,712\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_19 (\u001b[38;5;33mLSTM\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             â”‚           \u001b[38;5;34m528\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚            \u001b[38;5;34m51\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,712</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,707\u001b[0m (127.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,707</span> (127.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,707\u001b[0m (127.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,707</span> (127.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ë°ì´í„° ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©:\n",
            "  (0:ê±·ê¸°, 1:ë›°ê¸°, 2:ì •ì§€) â¡ï¸ {0: np.float64(0.7599840255591054), 1: np.float64(1.2746148693904888), 2: np.float64(1.1115654205607477)}\n",
            "\n",
            "--- 3-í´ë˜ìŠ¤ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ìˆ˜ì • ì ìš©) ---\n",
            "Epoch 1/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 47ms/step - accuracy: 0.4859 - loss: 5.2516 - val_accuracy: 0.9405 - val_loss: 1534.3326\n",
            "Epoch 2/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.8815 - loss: 928.1519 - val_accuracy: 0.9503 - val_loss: 22.7954\n",
            "Epoch 3/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.9192 - loss: 2667.6531 - val_accuracy: 0.9727 - val_loss: 13.9313\n",
            "Epoch 4/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.7713 - loss: 86926.3594 - val_accuracy: 0.7745 - val_loss: 128007.2812\n",
            "Epoch 5/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6033 - loss: 508566.5625 - val_accuracy: 0.6057 - val_loss: 6222.2998\n",
            "Epoch 6/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.6066 - loss: 12091.4932 - val_accuracy: 0.6800 - val_loss: 763.3568\n",
            "Epoch 7/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.5946 - loss: 1865.7471 - val_accuracy: 0.6380 - val_loss: 2501.7954\n",
            "Epoch 8/30\n",
            "\u001b[1m179/179\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.6108 - loss: 6798.2017 - val_accuracy: 0.6317 - val_loss: 3989.6453\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6325 - loss: 4481.6626\n",
            "\n",
            "--- í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ê²°ê³¼ ---\n",
            "  - ì†ì‹¤(Loss): 3989.6453\n",
            "  - ì •í™•ë„(Accuracy): 63.17%\n",
            "\n",
            "(ë¼ë²¨ 0: ê±·ê¸°, ë¼ë²¨ 1: ë›°ê¸°, ë¼ë²¨ 2: ì •ì§€)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step\n",
            "\n",
            "'ê±·ê¸°' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 0) â¡ï¸ [W, R, S] í™•ë¥ : [1. 0. 0.] â¡ï¸ ì˜ˆì¸¡: 0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\n",
            "'ë›°ê¸°' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 1) â¡ï¸ [W, R, S] í™•ë¥ : [0. 1. 0.] â¡ï¸ ì˜ˆì¸¡: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\n",
            "'ì •ì§€' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 2) â¡ï¸ [W, R, S] í™•ë¥ : [0.28467643 0.2468405  0.46848306] â¡ï¸ ì˜ˆì¸¡: 2\n",
            "íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ str_walk_20251106_200527.csvstr_wark2_20251106_200903.csv: [Errno 2] No such file or directory: 'str_walk_20251106_200527.csvstr_wark2_20251106_200903.csv'\n",
            "\n",
            "--- ë¼ì¦ˆë² ë¦¬íŒŒì´ìš© íŒŒì¼ Export ì‹œì‘ ---\n",
            "ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: multi_class_scaler.joblib\n",
            "Saved artifact at '/tmp/tmpv6v7v62u'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 12), dtype=tf.float32, name='keras_tensor_49')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132340232915472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232916432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232919120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232919696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232918928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232918352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232920272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232917392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232918544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340232917968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TFLite ëª¨ë¸ ì €ì¥ ì™„ë£Œ: multi_class_model.tflite\n",
            "\n",
            "--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---\n",
            "Colabì˜ 'íŒŒì¼' íƒ­ì—ì„œ 2ê°œì˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:\n",
            "1. multi_class_scaler.joblib\n",
            "2. multi_class_model.tflite\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# (ìˆ˜ì •) StandardScaler ëŒ€ì‹  RobustScaler ì„í¬íŠ¸\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# (ìˆ˜ì •) class_weight ê³„ì‚°ì„ ìœ„í•´ ì„í¬íŠ¸\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import joblib\n",
        "\n",
        "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
        "\n",
        "# 1. íŒŒì¼ ëª©ë¡ ì •ì˜ (3ê°€ì§€ í´ë˜ìŠ¤)\n",
        "# ë¼ë²¨ 0: ê±·ê¸°\n",
        "WALK_FILES = [\n",
        "    'nomal_wark3_20251106_201207.csv',\n",
        "    'normal_wark_20251106_195044.csv'\n",
        "]\n",
        "# ë¼ë²¨ 1: ë›°ê¸°\n",
        "RUN_FILES = [\n",
        "    'nomal_run_20251106_195716.csv',\n",
        "    'nomal_run2_20251106_200243.csv',\n",
        "    'nomal_run3_20251106_201900.csv',\n",
        "    'nomal_run6_20251106_203042.csv'\n",
        "]\n",
        "# ë¼ë²¨ 2: ì •ì§€\n",
        "STOP_FILES = [\n",
        "    'stop_20251106_213440.csv',\n",
        "    'stop2_20251106_213649.csv',\n",
        "    'stop3_20251106_215912.csv',\n",
        "    'stop4_20251106_220218.csv'\n",
        "\n",
        "]\n",
        "# í…ŒìŠ¤íŠ¸ìš© 'ì´ìƒí•œ ê±¸ìŒ'\n",
        "ABNORMAL_FILES = [\n",
        "    'str_walk_20251106_200527.csv'\n",
        "    'str_wark2_20251106_200903.csv' # ì´ íŒŒì¼ë„ ìˆë‹¤ë©´ ëª©ë¡ì— ì¶”ê°€\n",
        "]\n",
        "\n",
        "# 2. í—¬í¼ í•¨ìˆ˜\n",
        "def load_data(files):\n",
        "    \"\"\"íŒŒì¼ ëª©ë¡ì„ ë°›ì•„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.\"\"\"\n",
        "    df_list = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            df = pd.read_csv(f)\n",
        "            if not df.empty:\n",
        "                df_list.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {f}: {e}\")\n",
        "    if not df_list:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(df_list, axis=0, ignore_index=True)\n",
        "\n",
        "TIME_STEPS = 100\n",
        "def create_sequences(data, label, time_steps=TIME_STEPS):\n",
        "    \"\"\"ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ì™€ ë¼ë²¨ì„ ë°›ì•„ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(data) - time_steps + 1):\n",
        "        sequences.append(data[i:(i + time_steps)])\n",
        "        labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "print(\"\\n--- 3-í´ë˜ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---\")\n",
        "\n",
        "df_walk = load_data(WALK_FILES)\n",
        "df_run = load_data(RUN_FILES)\n",
        "df_stop = load_data(STOP_FILES)\n",
        "\n",
        "print(f\"ë¡œë“œëœ 'ê±·ê¸°' (ë¼ë²¨ 0) ë°ì´í„°: {len(df_walk)} í–‰\")\n",
        "print(f\"ë¡œë“œëœ 'ë›°ê¸°' (ë¼ë²¨ 1) ë°ì´í„°: {len(df_run)} í–‰\")\n",
        "print(f\"ë¡œë“œëœ 'ì •ì§€' (ë¼ë²¨ 2) ë°ì´í„°: {len(df_stop)} í–‰\")\n",
        "\n",
        "# (ì¤‘ìš”) ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ: 3ê°€ì§€ í´ë˜ìŠ¤ ë°ì´í„°ë¥¼ ëª¨ë‘ í•©ì³ì„œ fit\n",
        "df_all_raw = pd.concat([df_walk, df_run, df_stop], ignore_index=True)\n",
        "\n",
        "if df_all_raw.empty:\n",
        "    print(\"ì˜¤ë¥˜: í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì—…ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "else:\n",
        "    # *** FIX 1: StandardScalerë¥¼ RobustScalerë¡œ ë³€ê²½ (ì´ìƒì¹˜ ëŒ€ì‘) ***\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(df_all_raw)\n",
        "    print(\"ìŠ¤ì¼€ì¼ëŸ¬ê°€ 'ê±·ê¸°+ë›°ê¸°+ì •ì§€' ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. (RobustScaler ì‚¬ìš©)\")\n",
        "\n",
        "    # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©\n",
        "    scaled_walk = scaler.transform(df_walk)\n",
        "    scaled_run = scaler.transform(df_run)\n",
        "    scaled_stop = scaler.transform(df_stop)\n",
        "\n",
        "    # ë¼ë²¨ì„ ë¶™ì—¬ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    X_walk, y_walk = create_sequences(scaled_walk, 0) # 0 = Walk\n",
        "    X_run, y_run = create_sequences(scaled_run, 1)   # 1 = Run\n",
        "    X_stop, y_stop = create_sequences(scaled_stop, 2)   # 2 = Stop\n",
        "\n",
        "    print(f\"'ê±·ê¸°' ì‹œí€€ìŠ¤ ìƒì„±: {X_walk.shape}\")\n",
        "    print(f\"'ë›°ê¸°' ì‹œí€€ìŠ¤ ìƒì„±: {X_run.shape}\")\n",
        "    print(f\"'ì •ì§€' ì‹œí€€ìŠ¤ ìƒì„±: {X_stop.shape}\")\n",
        "\n",
        "    # 4. ì „ì²´ ë°ì´í„°ì…‹ ê²°í•© ë° ë¶„ë¦¬\n",
        "    X_full = np.concatenate((X_walk, X_run, X_stop))\n",
        "    y_full = np.concatenate((y_walk, y_run, y_stop))\n",
        "\n",
        "    # (ì¤‘ìš”) shuffle=Trueë¡œ 3ê°€ì§€ ë°ì´í„°ë¥¼ ê³¨ê³ ë£¨ ì„ìŒ\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_full, y_full, test_size=0.2, shuffle=True, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nì´ í•™ìŠµ ë°ì´í„°: {X_train.shape}, ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")\n",
        "    print(f\"ë¼ë²¨ ë¶„í¬ (í•™ìŠµ): {np.unique(y_train, return_counts=True)}\")\n",
        "    print(f\"ë¼ë²¨ ë¶„í¬ (í…ŒìŠ¤íŠ¸): {np.unique(y_test, return_counts=True)}\")\n",
        "\n",
        "    # 5. ë‹¤ì¤‘ í´ë˜ìŠ¤ LSTM ëª¨ë¸ êµ¬ì¶•\n",
        "    n_features = X_train.shape[2]\n",
        "    N_CLASSES = 3 # (0: ê±·ê¸°, 1: ë›°ê¸°, 2: ì •ì§€)\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(64, activation='relu', input_shape=(TIME_STEPS, n_features), return_sequences=True),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.LSTM(32, activation='relu', return_sequences=False),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(16, activation='relu'),\n",
        "        keras.layers.Dense(N_CLASSES, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # *** FIX 4: í•™ìŠµë¥ (Learning Rate)ì„ 0.0001ë¡œ ë‚®ì¶”ê³ , clipnorm=1.0 ìœ ì§€ ***\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
        "\n",
        "    model.compile(optimizer=optimizer, # ìˆ˜ì •ëœ ì˜µí‹°ë§ˆì´ì € ì ìš©\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # *** FIX 3 (Part 1): í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë°ì´í„° ë¶ˆê· í˜• í•´ì†Œ) ***\n",
        "    # y_train (í•™ìŠµ ë°ì´í„° ë¼ë²¨)ì„ ê¸°ë°˜ìœ¼ë¡œ í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
        "    class_weights = compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜ {0: 0.x, 1: 1.y, 2: 3.z}\n",
        "    class_weights_dict = dict(enumerate(class_weights))\n",
        "    print(f\"\\në°ì´í„° ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©:\")\n",
        "    print(f\"  (0:ê±·ê¸°, 1:ë›°ê¸°, 2:ì •ì§€) â¡ï¸ {class_weights_dict}\")\n",
        "\n",
        "    # 6. ëª¨ë¸ í•™ìŠµ\n",
        "    print(\"\\n--- 3-í´ë˜ìŠ¤ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ìˆ˜ì • ì ìš©) ---\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_test, y_test),\n",
        "        callbacks=[keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max')],\n",
        "        # *** FIX 3 (Part 2): class_weight ì˜µì…˜ ì¶”ê°€ ***\n",
        "        class_weight=class_weights_dict\n",
        "    )\n",
        "\n",
        "    # 7. ëª¨ë¸ í‰ê°€\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"\\n--- í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ê²°ê³¼ ---\")\n",
        "    print(f\"  - ì†ì‹¤(Loss): {loss:.4f}\")\n",
        "    print(f\"  - ì •í™•ë„(Accuracy): {accuracy * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\n(ë¼ë²¨ 0: ê±·ê¸°, ë¼ë²¨ 1: ë›°ê¸°, ë¼ë²¨ 2: ì •ì§€)\")\n",
        "    # 'ê±·ê¸°' ìƒ˜í”Œ(ë¼ë²¨ 0) ì˜ˆì¸¡\n",
        "    X_test_walk_sample = X_test[y_test == 0][:1]\n",
        "    if len(X_test_walk_sample) > 0:\n",
        "        pred = model.predict(X_test_walk_sample)\n",
        "        print(f\"\\n'ê±·ê¸°' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 0) â¡ï¸ [W, R, S] í™•ë¥ : {pred[0]} â¡ï¸ ì˜ˆì¸¡: {np.argmax(pred[0])}\")\n",
        "\n",
        "    # 'ë›°ê¸°' ìƒ˜í”Œ(ë¼ë²¨ 1) ì˜ˆì¸¡\n",
        "    X_test_run_sample = X_test[y_test == 1][:1]\n",
        "    if len(X_test_run_sample) > 0:\n",
        "        pred = model.predict(X_test_run_sample)\n",
        "        print(f\"\\n'ë›°ê¸°' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 1) â¡ï¸ [W, R, S] í™•ë¥ : {pred[0]} â¡ï¸ ì˜ˆì¸¡: {np.argmax(pred[0])}\")\n",
        "\n",
        "    # 'ì •ì§€' ìƒ˜í”Œ(ë¼ë²¨ 2) ì˜ˆì¸¡\n",
        "    X_test_stop_sample = X_test[y_test == 2][:1]\n",
        "    if len(X_test_stop_sample) > 0:\n",
        "        pred = model.predict(X_test_stop_sample)\n",
        "        print(f\"\\n'ì •ì§€' ìƒ˜í”Œ ì˜ˆì¸¡ (ì‹¤ì œ: 2) â¡ï¸ [W, R, S] í™•ë¥ : {pred[0]} â¡ï¸ ì˜ˆì¸¡: {np.argmax(pred[0])}\")\n",
        "\n",
        "    # 'ì´ìƒí•œ ê±¸ìŒ'ì€ ì–´ë–»ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ í™•ì¸\n",
        "    df_abnormal = load_data(ABNORMAL_FILES)\n",
        "    if not df_abnormal.empty:\n",
        "        scaled_abnormal = scaler.transform(df_abnormal)\n",
        "        X_abnormal, _ = create_sequences(scaled_abnormal, -1)\n",
        "        if len(X_abnormal) > 0:\n",
        "            pred_abnormal = model.predict(X_abnormal[:5])\n",
        "            print(f\"\\n--- 'ì´ìƒí•œ ê±¸ìŒ(Abnormal)' ë°ì´í„° ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ---\")\n",
        "            for i, pred in enumerate(pred_abnormal):\n",
        "                print(f\"  ìƒ˜í”Œ {i} â¡ï¸ [W, R, S]: {pred} â¡ï¸ ì˜ˆì¸¡: {np.argmax(pred)}\")\n",
        "\n",
        "    # 8. ìµœì¢… íŒŒì¼ Export\n",
        "    print(\"\\n--- ë¼ì¦ˆë² ë¦¬íŒŒì´ìš© íŒŒì¼ Export ì‹œì‘ ---\")\n",
        "\n",
        "    # (1) Scaler ì €ì¥ (ìƒˆë¡œìš´ RobustScaler)\n",
        "    scaler_filename = 'multi_class_scaler.joblib'\n",
        "    joblib.dump(scaler, scaler_filename)\n",
        "    print(f\"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_filename}\")\n",
        "\n",
        "    # (2) TFLite ëª¨ë¸ ì €ì¥ (ìƒˆë¡œìš´ ëª¨ë¸)\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "    # TFLite í˜¸í™˜ì„± ì„¤ì • (ì´ì „ ì„±ê³µ ì˜µì…˜)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS\n",
        "    ]\n",
        "    converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    tflite_filename = 'multi_class_model.tflite'\n",
        "    with open(tflite_filename, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    print(f\"TFLite ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {tflite_filename}\")\n",
        "\n",
        "    print(\"\\n--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---\")\n",
        "    print(\"Colabì˜ 'íŒŒì¼' íƒ­ì—ì„œ 2ê°œì˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:\")\n",
        "    print(\"1. multi_class_scaler.joblib\")\n",
        "    print(\"2. multi_class_model.tflite\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (TIME_STEPSëŠ” í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨. ì˜ˆ: TIME_STEPS = 100)\n",
        "\n",
        "# 1. ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜ (í•™ìŠµ ë•Œì™€ ë™ì¼)\n",
        "# (ì´ í•¨ìˆ˜ê°€ ì´ë¯¸ Colab ë…¸íŠ¸ë¶ ì–´ë”˜ê°€ì— ì •ì˜ë˜ì–´ ìˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)\n",
        "def create_sequences_for_inference(data, time_steps):\n",
        "    sequences = []\n",
        "    # ë°ì´í„°ê°€ time_stepsë³´ë‹¤ ì§§ìœ¼ë©´ ì˜ˆì¸¡ ë¶ˆê°€\n",
        "    if len(data) >= time_steps:\n",
        "        for i in range(len(data) - time_steps + 1):\n",
        "            sequences.append(data[i:(i + time_steps)])\n",
        "    return np.array(sequences)\n",
        "\n",
        "# 2. ìµœì¢… 'ì •ìƒë„' ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_running_accuracy(file_path, model_to_use, scaler_to_use, time_steps=100):\n",
        "    \"\"\"\n",
        "    ìƒˆë¡œìš´ CSV íŒŒì¼ í•˜ë‚˜ë¥¼ ë°›ì•„ 'ì •ìƒë„'ì˜ í‰ê·  ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "    :param file_path: ìƒˆ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ (ì˜ˆ: 'my_new_run.csv')\n",
        "    :param model_to_use: ì´ì „ì— í•™ìŠµëœ keras ëª¨ë¸ ê°ì²´\n",
        "    :param scaler_to_use: ì´ì „ì— í•™ìŠµëœ StandardScaler ê°ì²´\n",
        "    :param time_steps: í•™ìŠµ ë•Œ ì‚¬ìš©í•œ TIME_STEPS (ì˜ˆ: 100)\n",
        "    :return: 'ì •ìƒë„' í‰ê·  ì ìˆ˜ (0~100 ì‚¬ì´)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. ìƒˆ ë°ì´í„° ë¡œë“œ\n",
        "        df_new = pd.read_csv(file_path)\n",
        "\n",
        "        # 2. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ì¤‘ìš”: í•™ìŠµëœ scalerë¡œ 'transform'ë§Œ ìˆ˜í–‰)\n",
        "        scaled_new_data = scaler_to_use.transform(df_new)\n",
        "\n",
        "        # 3. ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "        new_sequences = create_sequences_for_inference(scaled_new_data, time_steps)\n",
        "\n",
        "        if len(new_sequences) == 0:\n",
        "            print(f\"  [ê²½ê³ ] '{file_path}' ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ì•„ (ì´ {len(df_new)} í–‰) ì˜ˆì¸¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ {time_steps} í–‰ í•„ìš”)\")\n",
        "            return 0.0 # ë˜ëŠ” None\n",
        "\n",
        "        # 4. ëª¨ë¸ ì˜ˆì¸¡ (ëª¨ë¸ì€ 'ë¹„ì •ìƒ(1)'ì¼ í™•ë¥ ì„ ë°˜í™˜)\n",
        "        # shape: (n_sequences, 1)\n",
        "        predictions_prob = model_to_use.predict(new_sequences)\n",
        "\n",
        "        # 5. 'ì •ìƒë„'ë¡œ ë³€í™˜\n",
        "        # (1 - ë¹„ì •ìƒ í™•ë¥ ) * 100 = ì •ìƒë„(%)\n",
        "        normality_scores = (1 - predictions_prob.flatten()) * 100\n",
        "\n",
        "        # 6. ì „ì²´ ì‹œí€€ìŠ¤ì˜ 'ì •ìƒë„' í‰ê·  ë°˜í™˜\n",
        "        average_score = np.mean(normality_scores)\n",
        "\n",
        "        return average_score\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  [ì˜¤ë¥˜] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "        return 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"  [ì˜¤ë¥˜] ì˜ˆì¸¡ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "03Q9JbsdDHp2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ì‹¤ì œ ì˜ˆì¸¡ ìˆ˜í–‰ ---\n",
        "\n",
        "# 1. (ê°€ì •) Colabì— ìƒˆë¡œìš´ íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "#    ì˜ˆì‹œë¡œ, í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í–ˆë˜ 'str_walk_20251106_200527.csv' íŒŒì¼ì„\n",
        "#    'my_test_abnormal.csv' ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì—…ë¡œë“œí–ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.\n",
        "#    (ì‹¤ì œë¡œëŠ” 'my_test_abnormal.csv' íŒŒì¼ì„ Colabì— ì—…ë¡œë“œí•´ì•¼ í•¨)\n",
        "\n",
        "# 2. (ê°€ì •) ì •ìƒ íŒŒì¼ë„ ì—…ë¡œë“œ\n",
        "#    'nomal_run2_20251106_200243.csv' íŒŒì¼ì„\n",
        "#    'my_test_normal.csv' ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì—…ë¡œë“œí–ˆë‹¤ê³  ê°€ì •\n",
        "\n",
        "# (modelê³¼ scalerëŠ” ì´ì „ ì½”ë“œ ì…€ì—ì„œ í•™ìŠµ ì™„ë£Œëœ ê°ì²´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ 1: 'ì •ìƒ' íŒŒì¼ ì˜ˆì¸¡\n",
        "file_to_test_1 = 'nomal_run2_20251106_200243.csv' # Colabì— ì—…ë¡œë“œëœ íŒŒì¼\n",
        "final_score_1 = predict_running_accuracy(file_to_test_1, model, scaler, TIME_STEPS)\n",
        "print(f\"\\n--- ì˜ˆì¸¡ ê²°ê³¼ 1 ---\")\n",
        "print(f\"íŒŒì¼ '{file_to_test_1}'ì˜ ìµœì¢… 'ì •ìƒë„': {final_score_1:.2f}%\")\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ 2: 'ë¹„ì •ìƒ' íŒŒì¼ ì˜ˆì¸¡\n",
        "file_to_test_2 = 'str_walk_20251106_200527.csv' # Colabì— ì—…ë¡œë“œëœ íŒŒì¼\n",
        "final_score_2 = predict_running_accuracy(file_to_test_2, model, scaler, TIME_STEPS)\n",
        "print(f\"\\n--- ì˜ˆì¸¡ ê²°ê³¼ 2 ---\")\n",
        "print(f\"íŒŒì¼ '{file_to_test_2}'ì˜ ìµœì¢… 'ì •ìƒë„': {final_score_2:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-lpm5ByHO-V",
        "outputId": "8ab77f2b-7ee3-4551-ff1c-7b620ff936ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "--- ì˜ˆì¸¡ ê²°ê³¼ 1 ---\n",
            "íŒŒì¼ 'nomal_run2_20251106_200243.csv'ì˜ ìµœì¢… 'ì •ìƒë„': 100.00%\n",
            "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "\n",
            "--- ì˜ˆì¸¡ ê²°ê³¼ 2 ---\n",
            "íŒŒì¼ 'str_walk_20251106_200527.csv'ì˜ ìµœì¢… 'ì •ìƒë„': 0.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib\n",
        "\n",
        "import joblib\n",
        "from google.colab import files\n",
        "import tensorflow as tf # tfê°€ importë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
        "\n",
        "# (model ê³¼ scaler ê°ì²´ëŠ” ì´ë¯¸ í•™ìŠµ/ìƒì„±ë˜ì—ˆë‹¤ê³  ê°€ì •)\n",
        "\n",
        "# --- 1. Scaler ì €ì¥ ---\n",
        "scaler_filename = 'running_scaler.joblib'\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "print(f\"Scaler ì €ì¥ ì™„ë£Œ: {scaler_filename}\")\n",
        "\n",
        "# --- 2. TFLite ëª¨ë¸ ë³€í™˜ ë° ì €ì¥ ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# (ì¤‘ìš”) ì„±ëŠ¥ ìµœì í™”\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# --- ğŸ’¡ (ì˜¤ë¥˜ ìˆ˜ì •) TensorList ops ë¬¸ì œ í•´ê²° ---\n",
        "# TFLiteê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì²˜ë¦¬ ëª»í•˜ëŠ” TF ì—°ì‚°(op)ì„ í¬í•¨í•˜ë„ë¡ í—ˆìš©\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS, # TFLite ê¸°ë³¸ ì—°ì‚°\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS    # TensorFlow ì—°ì‚° ì„ íƒì  í¬í•¨\n",
        "]\n",
        "# TensorList ê´€ë ¨ ë³€í™˜ ì˜µì…˜ ë¹„í™œì„±í™” (ì˜¤ë¥˜ ë©”ì‹œì§€ ê°€ì´ë“œ)\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "# --- (ìˆ˜ì • ì™„ë£Œ) ---\n",
        "\n",
        "print(\"TFLite ë³€í™˜ ì‹œì‘ (Select TF ops í¬í•¨)...\")\n",
        "tflite_model = converter.convert()\n",
        "print(\"TFLite ë³€í™˜ ì™„ë£Œ.\")\n",
        "\n",
        "# ë³€í™˜ëœ .tflite ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥\n",
        "tflite_filename = 'running_model.tflite'\n",
        "with open(tflite_filename, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(f\"TensorFlow Lite ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {tflite_filename}\")\n",
        "\n",
        "# --- 3. Colabì—ì„œ ë¡œì»¬ PCë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ---\n",
        "print(\"\\níŒŒì¼ 2ê°œë¥¼ ë¡œì»¬ PCë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
        "files.download(scaler_filename)\n",
        "files.download(tflite_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "tPTe0r7WH6rc",
        "outputId": "73712fc9-21c1-4442-f5ec-18827074db02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Scaler ì €ì¥ ì™„ë£Œ: running_scaler.joblib\n",
            "TFLite ë³€í™˜ ì‹œì‘ (Select TF ops í¬í•¨)...\n",
            "Saved artifact at '/tmp/tmpj8ueu4xw'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 12), dtype=tf.float32, name='keras_tensor_21')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132340035609680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132342706208208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132342706208400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132342706205520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132342706203216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340269758800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340269746704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340269744208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340269750736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132340269745744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TFLite ë³€í™˜ ì™„ë£Œ.\n",
            "TensorFlow Lite ëª¨ë¸ ì €ì¥ ì™„ë£Œ: running_model.tflite\n",
            "\n",
            "íŒŒì¼ 2ê°œë¥¼ ë¡œì»¬ PCë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b021703-a77d-4f2d-baf4-c0cbbf66b543\", \"running_scaler.joblib\", 1351)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ae8fdbf1-2692-4f46-b64c-77f4891ce133\", \"running_model.tflite\", 64584)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}